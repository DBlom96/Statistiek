{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c45f524b-f776-4e3e-9106-1c7188b4b8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222fcfa-f39d-4d56-98a2-ed37f9e723bc",
   "metadata": {},
   "source": [
    "# Correlatie en Regressie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938669e2-7730-4633-b481-24001f17e20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latex_table(X, Y, labels):\n",
    "    # Start building the LaTeX table\n",
    "    xlabel, ylabel = labels.values()\n",
    "    n = len(X)\n",
    "    alignment_chars = \"{c|\" + \"c\" * n + \"}\"\n",
    "    latex_code = f\"\"\"\n",
    "        \\\\begin{{center}}\n",
    "            \\\\begin{{tabular}}{alignment_chars}\n",
    "                \\\\toprule\n",
    "                    \\\\textbf{{{xlabel}}} {\" \".join([f\"& ${x}$\" for x in X])} \\\\\\\\\n",
    "                    \\\\textbf{{{ylabel}}} {\" \".join([f\"& ${y}$\" for y in Y])} \\\\\\\\\n",
    "                \\\\bottomrule\n",
    "            \\\\end{{tabular}}\n",
    "        \\\\end{{center}}\n",
    "    \"\"\"\n",
    "    \n",
    "    return latex_code\n",
    "\n",
    "def generate_latex_table_regression(X, Y):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "       \n",
    "    # Start building the LaTeX table\n",
    "    latex_code = \"\"\"\n",
    "        \\\\begin{center}\n",
    "            \\\\begin{tabular}{ccccc}\n",
    "                \\\\toprule\n",
    "                    $x$ & $y$ & $xy$ & $x^2$ & $y^2$ \\\\\\\\\n",
    "                \\\\midrule\n",
    "    \"\"\"\n",
    "    \n",
    "    # Populate table rows\n",
    "    for x, y, xy, x2, y2 in zip(X, Y, X * Y, X ** 2, Y ** 2):\n",
    "        x, y, xy, x2, y2 = map(lambda p: pretty_print(p), (x, y, xy, x2, y2))\n",
    "        latex_code += f\"\\t\\t${x}$ & ${y}$ & ${xy}$ & ${x2}$ & ${y2}$ \\\\\\\\\\n\"\n",
    "    \n",
    "    # Add final row with averages\n",
    "    # Compute averages\n",
    "    avg_X, avg_Y, avg_XY, avg_X2, avg_Y2 = map(lambda x: pretty_print(np.mean(x)), [X, Y, X * Y, X ** 2, Y ** 2])\n",
    "\n",
    "    latex_code += f\"\"\"\n",
    "                \\\\midrule\n",
    "                    $\\\\overline{{x}} = {avg_X}$ & $\\\\overline{{y}} = {avg_Y}$ & $\\\\overline{{xy}} = {avg_XY}$ & $\\\\overline{{x^2}} = {avg_X2}$ & $\\\\overline{{y^2}} = {avg_Y2}$ \\\\\\\\\n",
    "                \\\\bottomrule\n",
    "            \\\\end{{tabular}}\n",
    "        \\\\end{{center}}\n",
    "    \"\"\"    \n",
    "    return latex_code\n",
    "\n",
    "def regression_coefficients_latex(X, Y):  \n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    avg_X = np.mean(X)\n",
    "    avg_Y = np.mean(Y)\n",
    "    avg_XY = np.mean(X * Y)\n",
    "    avg_X_squared = np.mean(X ** 2)\n",
    "    \n",
    "    # Beta_1 (slope)\n",
    "    numerator = avg_XY - avg_X * avg_Y\n",
    "    denominator = avg_X_squared - avg_X**2\n",
    "    beta_1 = numerator / denominator\n",
    "    \n",
    "    # Beta_0 (intercept)\n",
    "    beta_0 = avg_Y - beta_1 * avg_X\n",
    "    \n",
    "    # LaTeX output\n",
    "    latex_code = f\"\"\"\n",
    "        \\\\begin{{align*}}\n",
    "            b &= \\\\frac{{\\\\overline{{xy}} - \\\\overline{{x}} \\\\cdot \\\\overline{{y}}}}{{\\\\overline{{x^2}} - (\\\\overline{{x}})^2}} \\\\\\\\\n",
    "              &= \\\\frac{{{pretty_print(avg_XY)} - {pretty_print(avg_X)} \\\\cdot {pretty_print(avg_Y)}}}{{{pretty_print(avg_X_squared)} - ({pretty_print(avg_X)})^2}} \\\\\\\\\n",
    "              &= \\\\frac{{{pretty_print(numerator)}}}{{{pretty_print(denominator)}}} \\\\approx {pretty_print(beta_1)} \\\\\\\\\n",
    "            a &= \\\\overline{{y}} - b \\\\cdot \\\\overline{{x}} \\\\\\\\\n",
    "              &= {pretty_print(avg_Y)} - {pretty_print(beta_1)} \\\\cdot {pretty_print(avg_X)} \\\\\\\\\n",
    "              &\\\\approx {pretty_print(beta_0)}.\n",
    "        \\\\end{{align*}}\n",
    "\n",
    "        De formule van de regressielijn behorende bij deze steekproef is dus gelijk aan $Y = {pretty_print(beta_0)}{beta_1:+.4f}X$.\n",
    "    \"\"\"\n",
    "    return latex_code, beta_0, beta_1\n",
    "\n",
    "\n",
    "def pearson_correlation_latex(X, Y):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    # Compute necessary statistical values\n",
    "    avg_X, avg_Y, avg_XY, avg_X_squared, avg_Y_squared = np.mean(X), np.mean(Y), np.mean(X*Y), np.mean(X ** 2), np.mean(Y ** 2)\n",
    "\n",
    "    \n",
    "    # Compute Pearson correlation coefficient\n",
    "    numerator = avg_XY - (avg_X * avg_Y)\n",
    "    denominator = np.sqrt((avg_X**2 - avg_X_squared) * (avg_Y**2 - avg_Y_squared))\n",
    "    r = numerator / denominator\n",
    "    \n",
    "    # Generate LaTeX output\n",
    "    latex_code = f\"\"\"\n",
    "        \\\\begin{{align*}}\n",
    "            r(x,y)  &= \\\\frac{{ \\\\overline{{x \\\\cdot y}} - \\\\overline{{x}} \\\\cdot \\\\overline{{y}} }}{{ \\\\sqrt{{ (\\\\overline{{x}}^2 - \\\\overline{{x^2}}) \\\\cdot (\\\\overline{{y}}^2 - \\\\overline{{y^2}}) }} }}\\\\\\\\\n",
    "                    &= \\\\frac{{ {pretty_print(avg_XY)} - {pretty_print(avg_X)} \\\\cdot {pretty_print(avg_Y)} }}{{ \\\\sqrt{{ ({pretty_print(avg_X)}^2 - {pretty_print(avg_X_squared)}) \\\\cdot ({pretty_print(avg_Y)}^{2} - {pretty_print(avg_Y_squared)}) }} }} \\\\\\\\\n",
    "                    &= \\\\frac{{{pretty_print(numerator)}}}{{{pretty_print(denominator)}}} \\\\\\\\\n",
    "                    &\\\\approx {pretty_print(r)}.\n",
    "        \\\\end{{align*}}\n",
    "    \"\"\"\n",
    "    return latex_code\n",
    "\n",
    "def spearman_correlation_latex(X, Y):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    if len(X) != len(Y):\n",
    "        raise ValueError(\"Beide lijsten moeten dezelfde lengte hebben.\")\n",
    "    \n",
    "    # Rangschik de gegevens\n",
    "    ranks_X, ranks_Y = rankdata(X), rankdata(Y)\n",
    "    print(ranks_X, ranks_Y)\n",
    "    \n",
    "    # Bereken de verschillen in rang\n",
    "    d = ranks_X - ranks_Y\n",
    "    d_squared = d ** 2\n",
    "    sum_d_squared = sum(d_squared)\n",
    "    n = len(X)\n",
    "    alignment_chars = \"c\" + \"c\" * n\n",
    "    \n",
    "    # Spearman's rho\n",
    "    numerator = 6 * np.sum(d_squared)\n",
    "    denominator = n * (n**2 - 1)\n",
    "    rho = 1 - (numerator / denominator)\n",
    "    \n",
    "    # Genereer LaTeX-uitvoer\n",
    "    latex_code = f\"\"\"\n",
    "        De eerste stap bij het berekenen van Spearman's correlatieco\\\\\"effici\\\\\"ent is het bepalen van de rankings van de uitkomsten voor $X$ en $Y$:\n",
    "        \\\\begin{{center}}\n",
    "            \\\\begin{{tabular}}{{{alignment_chars}}}\n",
    "                \\\\toprule\n",
    "                    {{\\\\bfseries Rangnummers $X$-waarden}} {\" \".join([f\"& ${i}$\" for i in ranks_X])} \\\\\\\\\n",
    "                    {{\\\\bfseries Rangnummers $Y$-waarden}} {\" \".join([f\"& ${i}$\" for i in ranks_Y])} \\\\\\\\\n",
    "                \\\\midrule\n",
    "                    {{\\\\bfseries Verschillen $d_i$}} {\" \".join([f\"& ${diff}$\" for diff in d])} \\\\\\\\\n",
    "                    {{\\\\bfseries Kwadratische verschillen $d_i^2$}} {\" \".join([f\"& ${diff2}$\" for diff2 in d_squared])} \\\\\\\\\n",
    "                \\\\bottomrule\n",
    "            \\\\end{{tabular}}\n",
    "        \\\\end{{center}}\n",
    "\n",
    "        De som van de kwadratische rangnummerverschillen is gelijk aan $\\\\sum_i d_i^2 = {sum_d_squared}$.\n",
    "        Aangezien de steekproefgrootte gelijk is aan $n = {n}$, is de rangcorrelatieco\\\\\"effici\\\\\"ent van Spearman gelijk aan\n",
    "        \\\\begin{{align*}}\n",
    "            r_s &= 1 - \\\\frac{{ 6 \\\\cdot \\\\sum_i d_i^2 }}{{ n^3 - n }}  \\\\\\\\\n",
    "                &= 1 - \\\\frac{{ 6 \\\\cdot {sum_d_squared} }}{{ {n}^3 - {n} }}  \\\\\\\\\n",
    "                &\\\\approx {pretty_print(rho)}.\n",
    "        \\\\end{{align*}}\n",
    "    \"\"\"\n",
    "    \n",
    "    return latex_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258f0156-721d-44d1-9c94-9e571b95756d",
   "metadata": {},
   "source": [
    "## Betrouwbaarheidsinterval voor $E[Y]$ en voorspellingsinterval voor $Y$ gegeven $X = x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5299c5f-9583-4110-ad82-9630857900e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_confidence_interval(X, Y, beta_0, beta_1, x0, confidence=0.95):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    n = len(X)\n",
    "    mean_X = np.mean(X)\n",
    "    mean_Y = np.mean(Y)\n",
    "    mean_X2 = np.mean(X ** 2)\n",
    "    mean_XY = np.mean(X * Y)\n",
    "    mean_Y2 = np.mean(Y ** 2)\n",
    "    confidence_percentage = int(100 * confidence)\n",
    "    \n",
    "    # 1. Compute the error terms and estimate the standard deviation of the error terms\n",
    "    error_terms = Y - (beta_0 + beta_1 * X)\n",
    "    sample_std_error = np.sqrt( np.sum( error_terms ** 2) / (n - 2) ) \n",
    "\n",
    "    # 2. Use the estimate to estimate the standard deviation of the mean Y given x\n",
    "    sample_std_mean = sample_std_error * np.sqrt( 1 / n * (1 + ((x0 - mean_X) ** 2 / (mean_X2 - mean_X ** 2))) )\n",
    "\n",
    "    y_pred = beta_0 + beta_1 * x0   \n",
    "    t_crit = t.ppf((1 + confidence) / 2, df=n - 2)\n",
    "    margin_error = t_crit * sample_std_mean\n",
    "\n",
    "    # Generate LaTeX output\n",
    "    latex_code = f\"\"\"\n",
    "        De eerste stap is om de puntschatting voor $Y$ te bepalen aan de hand van de regressielijn $Y = {pretty_print(beta_0)}{beta_1:+.4f}X$ door $X = {x0}$ in te vullen.\n",
    "        Dit geeft ons een puntschatting van $y_0 = {pretty_print(beta_0)} {beta_1:+.4f} \\\\cdot {x0} \\\\approx {pretty_print(y_pred)}$.\n",
    "        Daarnaast kunnen we de standaardafwijking $\\\\sigma$ van de storingsterm $\\\\varepsilon$ schatten:\n",
    "        \\\\begin{{align*}}\n",
    "            s_{{\\\\varepsilon}} &= \\\\sqrt{{ \\\\frac{{n}}{{n-2}} \\\\cdot \\\\left( \\\\overline{{y^2}} - a \\\\cdot \\\\overline{{y}} - b \\\\cdot \\\\overline{{xy}} \\\\right) }} \\\\\\\\ \n",
    "                               &= \\\\sqrt{{ \\\\frac{{{n}}}{{{n-2}}} \\\\cdot \\\\left( {pretty_print(mean_Y2)} - {pretty_print(beta_0)} \\\\cdot {pretty_print(mean_Y)} - {pretty_print(beta_1)} \\\\cdot {pretty_print(mean_XY)} \\\\right) }} \\\\\\\\ \n",
    "                               &\\\\approx {pretty_print(sample_std_error)}.\n",
    "        \\\\end{{align*}}\n",
    "\n",
    "        Vervolgens kunnen we een puntschatting berekenen van de standaardafwijking van de verwachtingswaarde van $Y$ voor gegeven $X = x_0$:\n",
    "        \\\\begin{{align*}}\n",
    "        s_{{\\\\mu}}  &= s_{{\\\\varepsilon}} \\\\cdot \\\\sqrt{{ \\\\frac{{1}}{{n}} \\\\cdot \\\\left( 1 + \\\\frac{{(x_0 - \\\\overline{{x}})^2}}{{\\\\overline{{x^2}} - \\\\overline{{x}}^2}} \\\\right) }} \\\\\\\\\n",
    "                    &= {sample_std_error} \\\\cdot \\\\sqrt{{ \\\\frac{{1}}{{{n}}} \\\\cdot \\\\left( 1 + \\\\frac{{({pretty_print(x0)} - {pretty_print(mean_X)})^2}}{{{pretty_print(mean_X2)} - {pretty_print(mean_X)}^2}} \\\\right) }} \\\\\\\\\n",
    "                    &\\\\approx {pretty_print(sample_std_mean)}.   \n",
    "        \\\\end{{align*}}\n",
    "\n",
    "        Omdat we de standaardafwijkingen geschat hebben en de storingstermen normaal verdeeld zijn, moeten we werken met de $t$-verdeling met $df = n - 2 = {n-2}$ vrijheidsgraden.\n",
    "        De $t$-waarde die hoort bij een betrouwbaarheidsniveau $\\\\alpha = {1-confidence}$ is gelijk aan\n",
    "        \\\\[\n",
    "            t = \\\\invt(\\\\text{{opp}} = 1 - \\\\alpha / 2; \\\\text{{df}} = n - 2) = \\\\invt(\\\\text{{opp}} = {1-alpha/2}; \\\\text{{df}} = {n-2}) \\\\approx {pretty_print(t_crit)}.\n",
    "        \\\\]\n",
    "        Het ${int(100 * confidence)}\\\\%$-betrouwbaarheidsinterval voor de gemiddelde $Y$ voor gegeven $X = x_0$ kan dus worden beschreven door\n",
    "        \\\\begin{{align*}}\n",
    "            &[y_0 - t \\\\cdot s_{{\\\\mu}}; y_0 - t \\\\cdot s_{{\\\\mu}}] \\\\\\\\\n",
    "            &= [{pretty_print(y_pred)} - {pretty_print(t_crit)} \\\\cdot {pretty_print(sample_std_mean)}; {pretty_print(y_pred)} + {pretty_print(t_crit)} \\\\cdot {pretty_print(sample_std_mean)}] \\\\\\\\\n",
    "            &\\\\approx [{pretty_print(y_pred - margin_error)}; {pretty_print(y_pred + margin_error)}].\n",
    "        \\\\end{{align*}}\n",
    "        \n",
    "        Met \\\\SI{{{confidence_percentage}}}{{\\\\percent}} betrouwbaarheid ligt het gemiddelde van $Y$ voor gegeven $X = {x0}$ tussen ${pretty_print(y_pred - margin_error)}$ en ${pretty_print(y_pred + margin_error)}$.       \n",
    "    \"\"\"\n",
    "    ci = (y_pred - margin_error, y_pred + margin_error)\n",
    "    return latex_code, y_pred, ci\n",
    "\n",
    "def regression_prediction_interval(X, Y, beta_0, beta_1, x0, confidence=0.95):\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    n = len(X)\n",
    "    mean_X = np.mean(X)\n",
    "    mean_Y = np.mean(Y)\n",
    "    mean_X2 = np.mean(X ** 2)\n",
    "    mean_XY = np.mean(X * Y)\n",
    "    mean_Y2 = np.mean(Y ** 2)\n",
    "    confidence_percentage = int(100 * confidence)\n",
    "\n",
    "    error_terms = Y - (beta_0 + beta_1 * X)\n",
    "    sample_std_error = np.sqrt( np.sum(error_terms ** 2) / (n - 2) )\n",
    "\n",
    "    sample_std_pred = sample_std_error * np.sqrt( 1 + 1 / n * (1 + ((x0 - mean_X) ** 2 / (mean_X2 - mean_X ** 2))) )\n",
    "\n",
    "    y_pred = beta_0 + beta_1 * x0\n",
    "    t_crit = t.ppf((1 + confidence) / 2, df=n - 2)\n",
    "    margin_error = t_crit * sample_std_pred\n",
    "    pi_left, pi_right = (y_pred - margin_error, y_pred + margin_error)\n",
    "    latex_code = f\"\"\"\n",
    "        De eerste stap is om de puntschatting voor $Y$ te bepalen aan de hand van de regressielijn $Y = {pretty_print(beta_0)}{beta_1:+.4f} \\\\cdot X$ door $X = {x0}$ in te vullen.\n",
    "        Dit geeft ons een puntschatting van $y_0 = {pretty_print(beta_0)} {beta_1:+.4f} \\\\cdot {x0} \\\\approx {pretty_print(y_pred)}$.\n",
    "        Daarnaast kunnen we de standaardafwijking $\\\\sigma$ van de storingsterm $\\\\varepsilon$ schatten:\n",
    "        \\\\begin{{align*}}\n",
    "            s_{{\\\\varepsilon}} &= \\\\sqrt{{ \\\\frac{{n}}{{n-2}} \\\\cdot \\\\left( \\\\overline{{y^2}} - a \\\\cdot \\\\overline{{y}} - b \\\\cdot \\\\overline{{xy}} \\\\right) }} \\\\\\\\ \n",
    "                               &= \\\\sqrt{{ \\\\frac{{{n}}}{{{n-2}}} \\\\cdot \\\\left( {pretty_print(mean_Y2)} - {pretty_print(beta_0)} \\\\cdot {pretty_print(mean_Y)} - {pretty_print(beta_1)} \\\\cdot {pretty_print(mean_XY)} \\\\right) }} \\\\\\\\ \n",
    "                               &\\\\approx {pretty_print(sample_std_error)}. \n",
    "        \\\\end{{align*}}\n",
    "\n",
    "        Vervolgens kunnen we een puntschatting berekenen van de standaardafwijking van $Y$ voor gegeven $X = x_0$:\n",
    "        \\\\begin{{align*}}\n",
    "            s_{{f}} &= s_{{\\\\varepsilon}} \\\\cdot \\\\sqrt{{ 1 + \\\\frac{{1}}{{n}} \\\\cdot \\\\left( 1 + \\\\frac{{(x_0 - \\\\overline{{x}})^2}}{{\\\\overline{{x^2}} - \\\\overline{{x}}^2}} \\\\right) }} \\\\\\\\\n",
    "                    &= {pretty_print(sample_std_error)} \\\\cdot \\\\sqrt{{ 1 + \\\\frac{{1}}{{{n}}} \\\\cdot \\\\left( 1 + \\\\frac{{({x0} - {pretty_print(mean_X)})^2}}{{{pretty_print(mean_X2)} - {pretty_print(mean_X)}^2}} \\\\right) }} \\\\\\\\\n",
    "                    &\\\\approx {pretty_print(sample_std_pred)}.         \n",
    "        \\\\end{{align*}}\n",
    "\n",
    "        Omdat we de standaardafwijkingen geschat hebben en de storingstermen normaal verdeeld zijn, moeten we werken met de $t$-verdeling met $df = n - 2 = {n-2}$ vrijheidsgraden.\n",
    "        De $t$-waarde die hoort bij een betrouwbaarheidsniveau $\\\\alpha = {1-confidence}$ is gelijk aan\n",
    "        \\\\[\n",
    "            t = \\\\invt(\\\\text{{opp}} = 1 - \\\\alpha / 2; \\\\text{{df}} = n - 2) = \\\\invt(\\\\text{{opp}} = {1-alpha/2}; \\\\text{{df}} = {n-2}) \\\\approx {pretty_print(t_crit)}.\n",
    "        \\\\]\n",
    "        Het \\\\SI{{{confidence_percentage}}}{{\\\\percent}}-betrouwbaarheidsinterval voor de gemiddelde $Y$ voor gegeven $X = x_0$ kan dus worden beschreven door\n",
    "        \\\\begin{{align*}}\n",
    "            &[y_0 - t \\\\cdot s_{{f}}; y_0 - t \\\\cdot s_{{f}}] \\\\\\\\ \n",
    "            &= [{pretty_print(y_pred)} - {pretty_print(t_crit)} \\\\cdot {pretty_print(sample_std_pred)}; {pretty_print(y_pred)} + {pretty_print(t_crit)} \\\\cdot {pretty_print(sample_std_pred)}] \\\\\\\\ \n",
    "            &\\\\approx [{pretty_print(y_pred - margin_error)}; {pretty_print(y_pred + margin_error)}]. \n",
    "        \\\\end{{align*}}\n",
    "        \n",
    "        Met \\\\SI{{{confidence_percentage}}}{{\\\\percent}} betrouwbaarheid ligt een toekomstige uitkomst van $Y$ voor gegeven $X = {x0}$ tussen ${pretty_print(pi_left)}$ en ${pretty_print(pi_right)}$.\n",
    "    \"\"\"\n",
    "\n",
    "    return latex_code, y_pred, (pi_left, pi_right)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af920de3-f8f9-4b77-a4bb-4cc2b048df43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_midpoint_lines_and_quadrants(mean_X, mean_Y):\n",
    "    left, right = plt.xlim()\n",
    "    bottom, top = plt.ylim()\n",
    "\n",
    "    plt.axvline(mean_X, linestyle=\"--\", color=secondary_plot_color)\n",
    "    plt.axhline(mean_Y, linestyle=\"--\", color=secondary_plot_color)\n",
    "\n",
    "    p, q = 0.2, 0.8\n",
    "    quadrant_labels = [\n",
    "        (p * mean_X + (1 - p) * right, q * mean_Y + (1 - q) * top, \"> 0\", \"> 0\"),\n",
    "        (p * mean_X + (1 - p) * left, q * mean_Y + (1 - q) * top, \"< 0\", \"> 0\"),\n",
    "        (p * mean_X + (1 - p) * left, q * mean_Y + (1 - q) * bottom, \"< 0\", \"< 0\"),\n",
    "        (p * mean_X + (1 - p) * right, q * mean_Y + (1 - q) * bottom, \"> 0\", \"< 0\"),\n",
    "    ]\n",
    "    for x, y, dx, dy in quadrant_labels:\n",
    "        plt.text(x, y, f\"$x_i - \\\\overline{{x}} {dx}$\\n$y_i - \\\\overline{{y}} {dy}$\", ha=\"center\", va=\"center\")\n",
    "\n",
    "def fit_regression_and_plot_line(X, Y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, Y)\n",
    "    x_vals = np.linspace(np.min(X), np.max(X), 200).reshape(-1, 1)\n",
    "    y_hat = model.predict(x_vals)\n",
    "\n",
    "    m, b = model.coef_[0], model.intercept_\n",
    "    label = f\"Regressielijn: $Y = {pretty_print(b)}{m:+.4f}X$\"\n",
    "    plt.plot(x_vals, y_hat, color=critical_color, label=label)\n",
    "\n",
    "    return model, x_vals.ravel(), y_hat\n",
    "\n",
    "def plot_residuals(X, Y, model):\n",
    "    for i, (xi, yi) in enumerate(zip(X[:, 0], Y)):\n",
    "        y_pred = model.predict([[xi]])[0]\n",
    "        label = \"Afstanden: $e_i = y_i - (a + b \\\\cdot x_i)$\" if i == 0 else None\n",
    "        plt.plot([xi, xi], [yi, y_pred], linestyle=\"--\", color=primary_plot_color, alpha=0.6, label=label)\n",
    "    \n",
    "def plot_y0(x_0, model):\n",
    "    left, right = plt.xlim()\n",
    "    bottom, top = plt.ylim()\n",
    "    y_0 = model.predict([[x_0]])[0]\n",
    "\n",
    "    plt.plot([x_0, x_0], [bottom, y_0], linestyle=\"--\", color=primary_plot_color)\n",
    "    plt.plot([left, x_0], [y_0, y_0], linestyle=\"--\", color=primary_plot_color,\n",
    "             label=f\"Puntschatting voor $Y \\\\mid X = {x_0}$: ${pretty_print(y_0)}$\")\n",
    "\n",
    "def plot_intervals(X, Y, x_vals, y_hat, model, x_0, plot_prediction_interval, plot_confidence_interval):\n",
    "    n = len(X)\n",
    "    x_mean = np.mean(X)\n",
    "    Sxx = np.sum((X - x_mean) ** 2)\n",
    "    residuals = Y - model.predict(X)\n",
    "    s_squared = np.sum(residuals ** 2) / (n - 2)\n",
    "    s = np.sqrt(s_squared)\n",
    "    t_val = t.ppf(0.975, df=n - 2)\n",
    "\n",
    "    se_mean = s * np.sqrt(1/n + (x_vals - x_mean)**2 / Sxx)\n",
    "    se_pred = s * np.sqrt(1 + 1/n + (x_vals - x_mean)**2 / Sxx)\n",
    "\n",
    "    ci_upper, ci_lower = y_hat + t_val * se_mean, y_hat - t_val * se_mean\n",
    "    pi_upper, pi_lower = y_hat + t_val * se_pred, y_hat - t_val * se_pred\n",
    "\n",
    "    if plot_prediction_interval:\n",
    "        plt.fill_between(x_vals, pi_lower, pi_upper, color=critical_color, alpha=0.2,\n",
    "                         label='Voorspellingsinterval (voor $Y \\\\mid X$)')\n",
    "        if x_0:\n",
    "            y_0 = model.predict([[x_0]])[0]\n",
    "            se_pred_0 = s * np.sqrt(1 + 1/n + (x_0 - x_mean)**2 / Sxx)\n",
    "            lower, upper = y_0 - t_val * se_pred_0, y_0 + t_val * se_pred_0\n",
    "            plt.plot([x_0, x_0], [lower, upper], color=critical_color, linestyle=\"--\", alpha=0.3,\n",
    "                     label=f'Voorspellingsinterval (voor $Y \\\\mid X={x_0}$): [{pretty_print(lower)}; {pretty_print(upper)}]')\n",
    "\n",
    "    if plot_confidence_interval:\n",
    "        plt.fill_between(x_vals, ci_lower, ci_upper, color=acceptable_color, alpha=0.3,\n",
    "                         label='Betrouwbaarheidsinterval (voor $E[Y \\\\mid X]$)')\n",
    "        if x_0:\n",
    "            y_0 = model.predict([[x_0]])[0]\n",
    "            se_mean_0 = s * np.sqrt(1/n + (x_0 - x_mean)**2 / Sxx)\n",
    "            lower, upper = y_0 - t_val * se_mean_0, y_0 + t_val * se_mean_0\n",
    "            plt.plot([x_0, x_0], [lower, upper], color=acceptable_color, linestyle=\"--\", alpha=0.3,\n",
    "                     label=f'Betrouwbaarheidsinterval (voor $E[Y \\\\mid X={x_0}$): [{pretty_print(lower)}; {pretty_print(upper)}]')\n",
    "            \n",
    "\n",
    "def plot_linear_regression(\n",
    "        X, Y, labels, filename, x_0=None,\n",
    "        plot_midpoint_lines=False, plot_least_squares=False, plot_regression_line=False, \n",
    "        plot_point_estimate=False,\n",
    "        plot_prediction_interval=False, plot_confidence_interval=False\n",
    "):\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X).reshape(-1, 1)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    mean_X, mean_Y = np.mean(X), np.mean(Y)\n",
    "\n",
    "    # Create scatter plot\n",
    "    plt.scatter(X, Y, color=primary_plot_color)\n",
    "    plt.title(f'Spreidingsdiagram: {labels[\"X\"]} vs. {labels[\"Y\"]}')\n",
    "    \n",
    "    # Label axes and add title\n",
    "    plt.xlabel(labels[\"X\"])\n",
    "    plt.ylabel(labels[\"Y\"])\n",
    "\n",
    "    if plot_midpoint_lines:\n",
    "        add_midpoint_lines_and_quadrants(mean_X, mean_Y)\n",
    "              \n",
    "    if plot_regression_line:\n",
    "        model, x_vals, y_hat = fit_regression_and_plot_line(X, Y)\n",
    "    \n",
    "        if plot_least_squares:\n",
    "            plot_residuals(X, Y, model)\n",
    "\n",
    "        if x_0 and plot_point_estimate:\n",
    "            plot_y0(x_0, model)\n",
    "    \n",
    "        if plot_prediction_interval or plot_confidence_interval:\n",
    "            plot_intervals(X, Y, x_vals, y_hat, model, x_0, plot_prediction_interval, plot_confidence_interval)\n",
    "\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), ncol=1)\n",
    "\n",
    "    \n",
    "    # Save plot\n",
    "    plt.savefig(filename, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e916c7db-d9a9-4f84-a638-8c51bcd3789c",
   "metadata": {},
   "source": [
    "### Voorbeeld gebruik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a992cc1a-e5a4-46fd-8c1a-ef4ad689dcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        \\begin{center}\n",
      "            \\begin{tabular}{c|ccccccccc}\n",
      "                \\toprule\n",
      "                    \\textbf{Slaaptijd (in uren)} & $4.0$ & $4.5$ & $5.0$ & $5.5$ & $6.0$ & $6.5$ & $7.0$ & $7.5$ & $8.0$ \\\\\n",
      "                    \\textbf{Hersteltijd na zware inspanning (in uren)} & $66$ & $61$ & $63$ & $62$ & $65$ & $64$ & $57$ & $59$ & $60$ \\\\\n",
      "                \\bottomrule\n",
      "            \\end{tabular}\n",
      "        \\end{center}\n",
      "    \n",
      "\n",
      "        \\begin{center}\n",
      "            \\begin{tabular}{ccccc}\n",
      "                \\toprule\n",
      "                    $x$ & $y$ & $xy$ & $x^2$ & $y^2$ \\\\\n",
      "                \\midrule\n",
      "    \t\t$4$ & $66$ & $264$ & $16$ & $4356$ \\\\\n",
      "\t\t$4.5$ & $61$ & $274.5$ & $20.25$ & $3721$ \\\\\n",
      "\t\t$5$ & $63$ & $315$ & $25$ & $3969$ \\\\\n",
      "\t\t$5.5$ & $62$ & $341$ & $30.25$ & $3844$ \\\\\n",
      "\t\t$6$ & $65$ & $390$ & $36$ & $4225$ \\\\\n",
      "\t\t$6.5$ & $64$ & $416$ & $42.25$ & $4096$ \\\\\n",
      "\t\t$7$ & $57$ & $399$ & $49$ & $3249$ \\\\\n",
      "\t\t$7.5$ & $59$ & $442.5$ & $56.25$ & $3481$ \\\\\n",
      "\t\t$8$ & $60$ & $480$ & $64$ & $3600$ \\\\\n",
      "\n",
      "                \\midrule\n",
      "                    $\\overline{x} = 6$ & $\\overline{y} = 61.8889$ & $\\overline{xy} = 369.1111$ & $\\overline{x^2} = 37.6667 & $\\overline{y^2} = 3837.8889$ \\\\\n",
      "                \\bottomrule\n",
      "            \\end{tabular}\n",
      "        \\end{center}\n",
      "    \n",
      "\n",
      "        \\begin{align*}\n",
      "            b &= \\frac{\\overline{xy} - \\overline{x} \\cdot \\overline{y}}{\\overline{x^2} - (\\overline{x})^2} \\\\\n",
      "              &= \\frac{369.1111 - 6 \\cdot 61.8889}{37.6667 - (6)^2} \\\\\n",
      "              &= \\frac{-2.2222}{1.6667} \\approx -1.3333 \\\\\n",
      "            a &= \\overline{y} - b \\cdot \\overline{x} \\\\\n",
      "              &= 61.8889 - -1.3333 \\cdot 6 \\\\\n",
      "              &\\approx 69.8889.\n",
      "        \\end{align*}\n",
      "\n",
      "        De formule van de regressielijn behorende bij deze steekproef is dus gelijk aan $Y = 69.8889-1.3333X$.\n",
      "    \n",
      "Voorspelde waarde voor $Y$ bij $X = 3.7$ is gelijk aan 64.95555555555556\n",
      "\n",
      "        \\begin{align*}\n",
      "            r(x,y)  &= \\frac{ \\overline{x \\cdot y} - \\overline{x} \\cdot \\overline{y} }{ \\sqrt{ (\\overline{x}^2 - \\overline{x^2}) \\cdot (\\overline{y}^2 - \\overline{y^2}) } }\\\\\n",
      "                    &= \\frac{ 369.1111 - 6 \\cdot 61.8889 }{ \\sqrt{ (6^2 - 37.6667) \\cdot (61.8889^2 - 3837.8889) } } \\\\\n",
      "                    &= \\frac{-2.2222}{3.5717} \\\\\n",
      "                    &\\approx -0.6222.\n",
      "        \\end{align*}\n",
      "    \n",
      "[1. 2. 3. 4. 5. 6. 7. 8. 9.] [9. 4. 6. 5. 8. 7. 1. 2. 3.]\n",
      "\n",
      "        De eerste stap bij het berekenen van Spearman's correlatieco\\\"effici\\\"ent is het bepalen van de rankings van de uitkomsten voor $X$ en $Y$:\n",
      "        \\begin{center}\n",
      "            \\begin{tabular}{cccccccccc}\n",
      "                \\toprule\n",
      "                    {\\bfseries Rangnummers $X$-waarden} & $1.0$ & $2.0$ & $3.0$ & $4.0$ & $5.0$ & $6.0$ & $7.0$ & $8.0$ & $9.0$ \\\\\n",
      "                    {\\bfseries Rangnummers $Y$-waarden} & $9.0$ & $4.0$ & $6.0$ & $5.0$ & $8.0$ & $7.0$ & $1.0$ & $2.0$ & $3.0$ \\\\\n",
      "                \\midrule\n",
      "                    {\\bfseries Verschillen $d_i$} & $-8.0$ & $-2.0$ & $-3.0$ & $-1.0$ & $-3.0$ & $-1.0$ & $6.0$ & $6.0$ & $6.0$ \\\\\n",
      "                    {\\bfseries Kwadratische verschillen $d_i^2$} & $64.0$ & $4.0$ & $9.0$ & $1.0$ & $9.0$ & $1.0$ & $36.0$ & $36.0$ & $36.0$ \\\\\n",
      "                \\bottomrule\n",
      "            \\end{tabular}\n",
      "        \\end{center}\n",
      "\n",
      "        De som van de kwadratische rangnummerverschillen is gelijk aan $\\sum_i d_i^2 = 196.0$.\n",
      "        Aangezien de steekproefgrootte gelijk is aan $n = 9$, is de rangcorrelatieco\\\"effici\\\"ent van Spearman gelijk aan\n",
      "        \\begin{align*}\n",
      "            r_s &= 1 - \\frac{ 6 \\cdot \\sum_i d_i^2 }{ n^3 - n }  \\\\\n",
      "                &= 1 - \\frac{ 6 \\cdot 196.0 }{ 9^3 - 9 }  \\\\\n",
      "                &\\approx -0.6333.\n",
      "        \\end{align*}\n",
      "    \n",
      "\n",
      "        De eerste stap is om de puntschatting voor $Y$ te bepalen aan de hand van de regressielijn $Y = 69.8889-1.3333X$ door $X = 1.45$ in te vullen.\n",
      "        Dit geeft ons een puntschatting van $y_0 = 69.8889 -1.3333 \\cdot 1.45 \\approx 67.9556$.\n",
      "        Daarnaast kunnen we de standaardafwijking $\\sigma$ van de storingsterm $\\varepsilon$ schatten:\n",
      "        \\begin{align*}\n",
      "            s_{\\varepsilon} &= \\sqrt{ \\frac{n}{n-2} \\cdot \\left( \\overline{y^2} - a \\cdot \\overline{y} - b \\cdot \\overline{xy} \\right) } \\\\ \n",
      "                               &= \\sqrt{ \\frac{9}{7} \\cdot \\left( 3837.8889 - 69.8889 \\cdot 61.8889 - -1.3333 \\cdot 369.1111 \\right) } \\\\ \n",
      "                               &\\approx 2.456.\n",
      "        \\end{align*}\n",
      "\n",
      "        Vervolgens kunnen we een puntschatting berekenen van de standaardafwijking van de verwachtingswaarde van $Y$ voor gegeven $X = x_0$:\n",
      "        \\begin{align*}\n",
      "        s_{\\mu}  &= s_{\\varepsilon} \\cdot \\sqrt{ \\frac{1}{n} \\cdot \\left( 1 + \\frac{(x_0 - \\overline{x})^2}{\\overline{x^2} - \\overline{x}^2} \\right) } \\\\\n",
      "                    &= 2.4559613253766908 \\cdot \\sqrt{ \\frac{1}{9} \\cdot \\left( 1 + \\frac{(1.45 - 6)^2}{37.6667 - 6^2} \\right) } \\\\\n",
      "                    &\\approx 2.9992.   \n",
      "        \\end{align*}\n",
      "\n",
      "        Omdat we de standaardafwijkingen geschat hebben en de storingstermen normaal verdeeld zijn, moeten we werken met de $t$-verdeling met $df = n - 2 = 7$ vrijheidsgraden.\n",
      "        De $t$-waarde die hoort bij een betrouwbaarheidsniveau $\\alpha = 0.050000000000000044$ is gelijk aan\n",
      "        \\[\n",
      "            t = \\invt(\\text{opp} = 1 - \\alpha / 2; \\text{df} = n - 2) = \\invt(\\text{opp} = 0.975; \\text{df} = 7) \\approx 2.3646.\n",
      "        \\]\n",
      "        Het $95\\%$-betrouwbaarheidsinterval voor de gemiddelde $Y$ voor gegeven $X = x_0$ kan dus worden beschreven door\n",
      "        \\begin{align*}\n",
      "            &[y_0 - t \\cdot s_{\\mu}; y_0 - t \\cdot s_{\\mu}] \\\\\n",
      "            &= [67.9556 - 2.3646 \\cdot 2.9992; 67.9556 + 2.3646 \\cdot 2.9992] \\\\\n",
      "            &\\approx [60.8637; 75.0475].\n",
      "        \\end{align*}\n",
      "        \n",
      "        Met \\SI{95}{\\percent} betrouwbaarheid ligt het gemiddelde van $Y$ voor gegeven $X = 1.45$ tussen $60.8637$ en $75.0475$.       \n",
      "    \n",
      "\n",
      "        De eerste stap is om de puntschatting voor $Y$ te bepalen aan de hand van de regressielijn $Y = 69.8889-1.3333 \\cdot X$ door $X = 1.45$ in te vullen.\n",
      "        Dit geeft ons een puntschatting van $y_0 = 69.8889 -1.3333 \\cdot 1.45 \\approx 67.9556$.\n",
      "        Daarnaast kunnen we de standaardafwijking $\\sigma$ van de storingsterm $\\varepsilon$ schatten:\n",
      "        \\begin{align*}\n",
      "            s_{\\varepsilon} &= \\sqrt{ \\frac{n}{n-2} \\cdot \\left( \\overline{y^2} - a \\cdot \\overline{y} - b \\cdot \\overline{xy} \\right) } \\\\ \n",
      "                               &= \\sqrt{ \\frac{9}{7} \\cdot \\left( 3837.8889 - 69.8889 \\cdot 61.8889 - -1.3333 \\cdot 369.1111 \\right) } \\\\ \n",
      "                               &\\approx 2.456. \n",
      "        \\end{align*}\n",
      "\n",
      "        Vervolgens kunnen we een puntschatting berekenen van de standaardafwijking van $Y$ voor gegeven $X = x_0$:\n",
      "        \\begin{align*}\n",
      "            s_{f} &= s_{\\varepsilon} \\cdot \\sqrt{ 1 + \\frac{1}{n} \\cdot \\left( 1 + \\frac{(x_0 - \\overline{x})^2}{\\overline{x^2} - \\overline{x}^2} \\right) } \\\\\n",
      "                    &= 2.456 \\cdot \\sqrt{ 1 + \\frac{1}{9} \\cdot \\left( 1 + \\frac{(1.45 - 6)^2}{37.6667 - 6^2} \\right) } \\\\\n",
      "                    &\\approx 3.8764.         \n",
      "        \\end{align*}\n",
      "\n",
      "        Omdat we de standaardafwijkingen geschat hebben en de storingstermen normaal verdeeld zijn, moeten we werken met de $t$-verdeling met $df = n - 2 = 7$ vrijheidsgraden.\n",
      "        De $t$-waarde die hoort bij een betrouwbaarheidsniveau $\\alpha = 0.050000000000000044$ is gelijk aan\n",
      "        \\[\n",
      "            t = \\invt(\\text{opp} = 1 - \\alpha / 2; \\text{df} = n - 2) = \\invt(\\text{opp} = 0.975; \\text{df} = 7) \\approx 2.3646.\n",
      "        \\]\n",
      "        Het \\SI{95}{\\percent}-betrouwbaarheidsinterval voor de gemiddelde $Y$ voor gegeven $X = x_0$ kan dus worden beschreven door\n",
      "        \\begin{align*}\n",
      "            &[y_0 - t \\cdot s_{f}; y_0 - t \\cdot s_{f}] \\\\ \n",
      "            &= [67.9556 - 2.3646 \\cdot 3.8764; 67.9556 + 2.3646 \\cdot 3.8764] \\\\ \n",
      "            &\\approx [58.7892; 77.1219]. \n",
      "        \\end{align*}\n",
      "        \n",
      "        Met \\SI{95}{\\percent} betrouwbaarheid ligt een toekomstige uitkomst van $Y$ voor gegeven $X = 1.45$ tussen $58.7892$ en $77.1219$.\n",
      "    \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(42)  # Zet de seed\n",
    "X = [4.0, 4.5, 5.0, 5.5, 6.0, 6.5, 7.0, 7.5, 8.0]\n",
    "            # + np.random.normal(loc=0, scale=40, size=len(Y))# oefeningstijd (min)\n",
    "Y = [66, 61, 63, 62, 65, 64, 57, 59, 60]\n",
    "\n",
    "X_pred = 6.75\n",
    "labels = {\n",
    "    \"X\": \"Slaaptijd (in uren)\",\n",
    "    \"Y\": \"Hersteltijd na zware inspanning (in uren)\"\n",
    "}\n",
    "\n",
    "# Generate LaTeX table for the question itself (horizontal)\n",
    "latex_table = generate_latex_table(X, Y, labels)\n",
    "print(latex_table)\n",
    "\n",
    "# Plot X and Y in a scatter plot\n",
    "plot_linear_regression(X, Y, labels, x_0 = X_pred, \\\n",
    "                           plot_least_squares=False, plot_regression_line=False, \\\n",
    "                           plot_point_estimate=False, plot_prediction_interval=False,\\\n",
    "                           filename=FIGURE_PATH + \"20250725_q4_scatterplot.png\")\n",
    "\n",
    "# Generate LaTeX table for computing the regression line and the correlation coefficient\n",
    "regression_table = generate_latex_table_regression(X, Y)\n",
    "print(regression_table)\n",
    "\n",
    "regression_coefficients, a, b = regression_coefficients_latex(X, Y)\n",
    "print(regression_coefficients)\n",
    "\n",
    "Y_pred = a + b * X_pred\n",
    "print(f\"Voorspelde waarde voor $Y$ bij $X = {X_pred}$ is gelijk aan {Y_pred}\")\n",
    "\n",
    "pearson = pearson_correlation_latex(X, Y)\n",
    "print(pearson)\n",
    "\n",
    "spearman = spearman_correlation_latex(X, Y)\n",
    "print(spearman)\n",
    "\n",
    "# Confidence interval for the mean given x0\n",
    "x0 = 6.75\n",
    "alpha = 0.05\n",
    "\n",
    "confidence_interval, y_pred, ci = regression_confidence_interval(X, Y, a, b, x0, confidence=1-alpha)\n",
    "print(confidence_interval)\n",
    "\n",
    "prediction_interval, y_pred, pi = regression_prediction_interval(X, Y, a, b, x0, confidence=1-alpha)\n",
    "print(prediction_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcc640-cef7-4920-9f4c-f8bca7ed3edf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acee86d8-d6a9-4113-a430-a30035625420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
